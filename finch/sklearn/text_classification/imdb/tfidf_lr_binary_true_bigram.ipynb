{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tfidf_lr_binary_true_bigram.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"AsflpquGwz_n","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import time\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZO1lddnxeGt","colab_type":"code","colab":{}},"source":["def get_data():\n","  _word2idx = tf.keras.datasets.imdb.get_word_index()\n","  word2idx = {w: i+3 for w, i in _word2idx.items()}\n","  word2idx['<pad>'] = 0\n","  word2idx['<start>'] = 1\n","  word2idx['<unk>'] = 2\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  \n","  (X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data()\n","  x_train = [' '.join([idx2word[i] for i in document][1:]) for document in X_train]\n","  x_test = [' '.join([idx2word[i] for i in document][1:]) for document in X_test]\n","  return (x_train, y_train), (x_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKjlPoUwzvYu","colab_type":"code","outputId":"72469259-2145-4943-ca86-7bc2f02baeb2","executionInfo":{"status":"ok","timestamp":1590820231728,"user_tz":-480,"elapsed":61458,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["(x_train, y_train), (x_test, y_test) = get_data()\n","\n","count_model = CountVectorizer(binary=True, ngram_range=(1,2))\n","count_model.fit(x_train)\n","\n","tfidf_model = TfidfTransformer()\n","tfidf_model.fit(count_model.transform(x_train))\n","X_train_tfidf = tfidf_model.transform(count_model.transform(x_train))\n","X_test_tfidf = tfidf_model.transform(count_model.transform(x_test))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wkIMtUOe1zKH","colab_type":"code","outputId":"c1fdaca5-c735-4e2d-c275-975cc6bd700f","executionInfo":{"status":"ok","timestamp":1590820433144,"user_tz":-480,"elapsed":20578,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["lr_model = LogisticRegression(solver='lbfgs')\n","y_pred = lr_model.fit(X_train_tfidf, y_train).predict(X_test_tfidf)\n","final_acc = (y_pred == y_test).mean()\n","print(\"final testing accuracy: {:.4f}\".format(final_acc))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["final testing accuracy: 0.8957\n"],"name":"stdout"}]}]}