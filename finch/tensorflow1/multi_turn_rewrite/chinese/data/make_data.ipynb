{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"NjOsfqIsbMMx","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow1/multi_turn_rewrite/chinese/data')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ByN906gKdFnm","colab_type":"code","colab":{}},"source":["from pathlib import Path\n","from collections import Counter\n","\n","import random\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJqILD1WbuLw","colab_type":"code","outputId":"a510632a-b4fd-4818-96c4-258dd5949db6","executionInfo":{"status":"ok","timestamp":1587803187558,"user_tz":-480,"elapsed":8497,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":150}},"source":["Path('../vocab').mkdir(exist_ok=True)\n","counter_char = Counter()\n","\n","with open('corpus.txt') as f, open('train_pos.txt', 'w') as f_train_pos, open('test_pos.txt', 'w') as f_test_pos, open('train_neg.txt', 'w') as f_train_neg, open('test_neg.txt', 'w') as f_test_neg:\n","  for line in f:\n","    line = line.rstrip().lower()\n","    try:\n","      h1, h2, q, a = [seg for seg in line.split('\\t') if seg != '']\n","    except:\n","      print('Deleted incorrect data:', line)\n","      continue\n","    if a == 'a' or a == '哆啦a梦' or a == '分手了很难过':\n","      print('Deleted incorrect data:', line)\n","      continue\n","    if random.random() < 0.05:\n","      f_test_pos.write(h1+'\\t'+h2+'\\t'+q+'\\t'+a+'\\n')\n","      f_test_neg.write(h1+'\\t'+h2+'\\t'+a+'\\t'+a+'\\n')\n","    else:\n","      f_train_pos.write(h1+'\\t'+h2+'\\t'+q+'\\t'+a+'\\n')\n","      f_train_neg.write(h1+'\\t'+h2+'\\t'+a+'\\t'+a+'\\n')\n","    counter_char.update(list(h1)+list(h2)+list(q)+list(a))\n","\n","most_common = lambda x: [w for w, freq in x.most_common()]\n","chars = ['<pad>', '<start>', '<end>'] + most_common(counter_char)\n","print(len(chars), 'Vocabulary')\n","\n","with open('../vocab/char.txt', 'w') as f:\n","  for char in chars:\n","    f.write(char+'\\n')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Deleted incorrect data: 晚上需要开空调吗\t\t\t\t回答我\t\t回答我什么时候开始晴天\n","Deleted incorrect data: 那你认识张琪吗\t\t额是的\t\t她长什么样\t\ta\n","Deleted incorrect data: 诗乃怎么样\t\t诗乃是不错 \t\t哪里不错\t\ta\n","Deleted incorrect data: 第五元素最喜欢的吕克贝松电影之一另一部是圣女贞德 \t\t\t\t我看过这个电视剧了\t\t我看过第五元素电视剧了\n","Deleted incorrect data: 哆啦a梦\t\t蓝梦岛金银岛\t\t是新出来的电影\t\t哆啦a梦\n","Deleted incorrect data: 为啥遇到什么事了\t\t分手了\t\t安啦你一定会找到那个对的人的\t\t分手了很难过\n","3852 Vocabulary\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CF1zrD6AjEKv","colab_type":"code","outputId":"bdb07320-721d-437e-f4fb-e2ed8ff023f9","executionInfo":{"status":"ok","timestamp":1587803259910,"user_tz":-480,"elapsed":69568,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":436}},"source":["char2idx = {}\n","with open('../vocab/char.txt') as f:\n","  for i, line in enumerate(f):\n","    line = line.rstrip('\\n')\n","    char2idx[line] = i\n","\n","embedding = np.zeros((len(char2idx)+1, 300)) # + 1 for unknown word\n","\n","with open('../vocab/cc.zh.300.vec') as f:\n","  count = 0\n","  for i, line in enumerate(f):\n","    if i == 0:\n","      continue\n","    if i % 100000 == 0:\n","      print('- At line {}'.format(i))\n","    line = line.rstrip()\n","    sp = line.split(' ')\n","    word, vec = sp[0], sp[1:]\n","    if word in char2idx:\n","      count += 1\n","      embedding[char2idx[word]] = np.asarray(vec, dtype='float32')\n","      \n","print(\"[%d / %d] characters have found pre-trained values\"%(count, len(char2idx)))\n","np.save('../vocab/char.npy', embedding)\n","print('Saved ../vocab/char.npy')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["- At line 100000\n","- At line 200000\n","- At line 300000\n","- At line 400000\n","- At line 500000\n","- At line 600000\n","- At line 700000\n","- At line 800000\n","- At line 900000\n","- At line 1000000\n","- At line 1100000\n","- At line 1200000\n","- At line 1300000\n","- At line 1400000\n","- At line 1500000\n","- At line 1600000\n","- At line 1700000\n","- At line 1800000\n","- At line 1900000\n","- At line 2000000\n","[3763 / 3852] characters have found pre-trained values\n","Saved ../vocab/char.npy\n"],"name":"stdout"}]}]}