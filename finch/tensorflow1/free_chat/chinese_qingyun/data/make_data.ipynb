{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNKlFAAmOtDsmJwtcuPBtMw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"D9BYUhyoWn_r","colab_type":"code","outputId":"c6957210-a2bb-4557-a9a3-7fb543e0be87","executionInfo":{"status":"ok","timestamp":1586921773808,"user_tz":-480,"elapsed":1665,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow1/free_chat/chinese_qingyun/data')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gvnDlghfbPS2","colab_type":"code","colab":{}},"source":["from collections import Counter\n","from pathlib import Path\n","from zh_wiki import zh2Hant\n","\n","import numpy as np\n","import random\n","import re"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TGCf1kNnWx_O","colab_type":"code","outputId":"d66721a4-6fe3-4868-c9c4-d729ae3c9943","executionInfo":{"status":"ok","timestamp":1586921791316,"user_tz":-480,"elapsed":4583,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["Path('../vocab').mkdir(exist_ok=True)\n","char_counter = Counter()\n","src_lens, tgt_lens = [], []\n","\n","intab, outtab = [], []\n","for s, t in zh2Hant.items():\n","  if len(s) == 1 and len(t) == 1:\n","    intab.append(t)\n","    outtab.append(s)\n","trantab = str.maketrans(''.join(intab), ''.join(outtab))\n","\n","with open('./raw_data.csv') as f, open('./train.txt', 'w') as f_tr, open('./test.txt', 'w') as f_te:\n","  for line in f:\n","    line = line.rstrip().lower()\n","    line = re.sub('{.*}', ' ', line)\n","    line = line.replace('★', ' ')\n","    line = re.sub('\\s+', ' ', line)\n","    if ('我的粉丝也不是' in line) or ('qq' in line) or ('菲菲' in line) or ('飲水得喇' in line):\n","      continue\n","    line = line.translate(trantab)\n","    src, tgt = line.split(' | ')\n","    src = src.strip()\n","    tgt = tgt.strip()\n","    if len(src) > 0 and len(tgt) > 0:\n","      if random.random() < 0.03:\n","        f_te.write(src+'<SEP>'+tgt+'\\n')\n","      else:\n","        f_tr.write(src+'<SEP>'+tgt+'\\n')\n","      char_counter.update(list(src))\n","      char_counter.update(list(tgt))\n","      src_lens.append(len(src))\n","      tgt_lens.append(len(tgt))\n","\n","print('Source Average Length', sum(src_lens)/len(src_lens))\n","print('Target Average Length', sum(tgt_lens)/len(tgt_lens))\n","\n","chars = ['<pad>', '<start>', '<end>'] + [char for char, freq in char_counter.most_common() if freq >= 5]\n","print(len(chars), 'Chars')\n","with open('../vocab/char.txt', 'w') as f:\n","  for c in chars:\n","    f.write(c+'\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Source Average Length 6.726910849536641\n","Target Average Length 10.60665363797653\n","3475 Chars\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-JsCMhBmcu6_","colab_type":"code","outputId":"66aa364d-0c7e-4d1c-954a-dec8b7d1e8ef","executionInfo":{"status":"ok","timestamp":1586921882294,"user_tz":-480,"elapsed":83862,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":436}},"source":["char2idx = {}\n","with open('../vocab/char.txt') as f:\n","  for i, line in enumerate(f):\n","    line = line.rstrip('\\n')\n","    char2idx[line] = i\n","\n","embedding = np.zeros((len(char2idx)+1, 300)) # + 1 for unknown word\n","\n","with open('../vocab/cc.zh.300.vec') as f:\n","  count = 0\n","  for i, line in enumerate(f):\n","    if i == 0:\n","      continue\n","    if i % 100000 == 0:\n","      print('- At line {}'.format(i))\n","    line = line.rstrip()\n","    sp = line.split(' ')\n","    word, vec = sp[0], sp[1:]\n","    if word in char2idx:\n","      count += 1\n","      embedding[char2idx[word]] = np.asarray(vec, dtype='float32')\n","      \n","print(\"[%d / %d] characters have found pre-trained values\"%(count, len(char2idx)))\n","np.save('../vocab/char.npy', embedding)\n","print('Saved ../vocab/char.npy')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["- At line 100000\n","- At line 200000\n","- At line 300000\n","- At line 400000\n","- At line 500000\n","- At line 600000\n","- At line 700000\n","- At line 800000\n","- At line 900000\n","- At line 1000000\n","- At line 1100000\n","- At line 1200000\n","- At line 1300000\n","- At line 1400000\n","- At line 1500000\n","- At line 1600000\n","- At line 1700000\n","- At line 1800000\n","- At line 1900000\n","- At line 2000000\n","[3417 / 3475] characters have found pre-trained values\n","Saved ../vocab/char.npy\n"],"name":"stdout"}]}]}